# DQN模型结构的选择
## State->actions Value 结构
State->actions Value 结构，指的是策略函数输入是状态向量，输出是值向量  
其中值函数向量中每一个元素代表了对应动作下的值  

在实际操作中，策略函数获取当前状态向量，计算出所有动作的值向量之后，需要遍历这个值向量，选择最大的值，再索引到动作，作为agent的动作输出  

这种策略函数结构有两个特点：  

1. 这种结构仅适用于环境模型可重置的情况  

原因在于，在训练该策略函数时，需要生成满足结构要求的输入-输出样本对，输入直接采集状态向量就可以了，但是输出样本，需要根据Q学习迭代公式来计算每个动作的值，其中需要计算每一个动作对应的nextState的Q值  
对于环境模型可重置的情况，可以在当前状态下，不断访问并重置，遍历所有动作，根据各个动作作用于环境得到的nextState，计算Q值，最后生成输出样本。只有当一组输入-输出样本对全部生成之后，才能用于建模。因此，如果离散动作有n种，则生成一个训练样本需要重置环境模型n次  

2. 这种结构仅适用于离散控制问题  

State->actions Value结构，由于策略函数输出的维度对应了动作的数量，而维度必然是离散的，所以这种结构只能用于离散动作控制。  

## [State action]->Value 结构
[State action]->Value 结构，指的是策略函数输入不仅有当前状态向量，还包含了当前的动作向量，输出是当前状态下执行该动作的值  

这中策略函数结构相对与State->actions Value 结构来说：  

1. 这种结构可用于在线学习  

对于在线学习面对的问题，其环境模型往往是不可重置的(有时甚至是无模型的)。当Agent执行一个动作之后，环境状态发生变化，且Agent只能继续在新的状态执行下一个动作，而无法重置环境状态以尝试其他动作。  
对于[State action]->Value 结构，需要收集当前状态和所执行的动作作为一个样本的输入，另外需要额外收集所到达的新状态nextState，并计算Q值作为一个样本的输出。  

2. 这种结构可用于连续控制问题  

由于动作是作为策略函数的输入，而策略函数对输入是没有限制的。无论输入任何[状态 动作]向量，均可以反馈其Q值。  
连续控制的难点在于给定状态的最优动作的计算上，求解最优动作本质上是一个最优化问题： 

$$
A_{max}=argmax(Q=f(S,A))
$$

决策过程，每次Agent在执行动作前都需要求解该最优化问题。训练过程，每个样本求解nextState所有动作的最大Q值时也需要求解该最优化问题，样本数越多，求解耗时也越大。因此对该最优化问题的求解效率要求较高。







